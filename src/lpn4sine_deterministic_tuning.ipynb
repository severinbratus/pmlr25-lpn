{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90cd0a3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "import wandb\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6041d049",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sine_data_generator import SineDataGenerator\n",
    "from models.decoder import Decoder\n",
    "from models.utils import ReluNet, make_leave_one_out\n",
    "from models.encoder import DeterministicIOPairSetEncoder\n",
    "from models.lpn import DeterministicLPN\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "663549f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(1)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "n_metaiter = 30_001\n",
    "n_samples_per_task = 20\n",
    "batch_size = 25\n",
    "d_latent = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0875365",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training function\n",
    "def train_deterministic_lpn(lpn, data_gen, device):\n",
    "    wandb.init(project=\"deterministic_lpn4sine_cf_maml\")\n",
    "        \n",
    "    n_metaiter = 30_001 \n",
    "    n_samples_per_task = 20\n",
    "    batch_size = 25\n",
    "    numstep = 2\n",
    "    d_latent = lpn.d_latent\n",
    "    \n",
    "    print(\"Total parameters:\", sum(p.numel() for p in lpn.parameters()))\n",
    "    \n",
    "    optimizer = torch.optim.AdamW(lpn.parameters(), lr=1e-3)\n",
    "    \n",
    "    losses = []\n",
    "    \n",
    "    for i in range(n_metaiter):\n",
    "        lpn.train()\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        xs, ys, amp, phase = data_gen.generate()\n",
    "        io_pairs = torch.cat([xs, ys], dim=-1).to(device)  # (B, N, 2)\n",
    "        \n",
    "        _, loss = lpn(io_pairs, K=numstep, debug=(i==0))\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if i % 100 == 0:\n",
    "            wandb.log({\"loss\": loss.item()})\n",
    "            losses.append(loss.item())\n",
    "        if i % 1000 == 0:\n",
    "            print(f\"Step {i}, Loss: {loss.item():.4f}\")\n",
    "    \n",
    "    # Save the model\n",
    "    torch.save(lpn.state_dict(), f'deterministic_lpn_H{d_latent}_GA{numstep}.pth')\n",
    "    wandb.finish()\n",
    "    \n",
    "    return losses\n",
    "\n",
    "# Evaluation function\n",
    "def evaluate_deterministic_lpn(lpn, device):\n",
    "    n_test_points = 600\n",
    "    test_batch_size = 1\n",
    "    n_test_samples_per_task = 20\n",
    "    n_support_samples_per_task = 10\n",
    "    n_query_samples_per_task = n_test_samples_per_task - n_support_samples_per_task\n",
    "    \n",
    "    test_numstep = 10 #100\n",
    "    data_gen = SineDataGenerator(n_test_samples_per_task, test_batch_size)\n",
    "    \n",
    "    \n",
    "    metaeval_losses = np.full((n_test_points, test_numstep + 1), np.nan)\n",
    "    \n",
    "    for i_test_task in range(n_test_points):\n",
    "        test_xs, test_ys, amp, phase = data_gen.generate()\n",
    "        support_xs = test_xs[:, :n_support_samples_per_task, :]\n",
    "        support_ys = test_ys[:, :n_support_samples_per_task, :]\n",
    "        query_xs = test_xs[:, n_support_samples_per_task:, :]\n",
    "        query_ys = test_ys[:, n_support_samples_per_task:, :]\n",
    "        \n",
    "        support_pairs = torch.cat([support_xs, support_ys], dim=-1).to(device)\n",
    "        aux_t, loss_t = lpn(support_pairs, K=test_numstep, debug=True)\n",
    "        \n",
    "        # Note: Different key names from the probabilistic version\n",
    "        z_traj = aux_t[\"z_traj\"]\n",
    "        print(len(z_traj))\n",
    "        print(test_numstep + 1)\n",
    "        assert len(z_traj) == test_numstep + 1\n",
    "        for i_step, z in enumerate(z_traj):\n",
    "            z = z.mean(dim=1).squeeze(0)  # (d_latent,)\n",
    "            \n",
    "            # Evaluate MSE loss of z on the query set\n",
    "            query_ys_pred = lpn.decode(z, query_xs.squeeze(0).to(device))\n",
    "            mse = nn.functional.mse_loss(query_ys_pred, query_ys.to(device), reduction='sum')\n",
    "            metaeval_losses[i_test_task, i_step] = mse.item()\n",
    "    \n",
    "    # Calculate confidence intervals correctly\n",
    "    n_samples = metaeval_losses.shape[0]\n",
    "    metaeval_mu = metaeval_losses.mean(axis=0)\n",
    "    metaeval_ci = 1.96 * metaeval_losses.std(axis=0) / np.sqrt(n_samples)\n",
    "    \n",
    "    lower_err = np.minimum(metaeval_ci, metaeval_mu)  # ensures lower bound â‰¥ 0\n",
    "    upper_err = metaeval_ci\n",
    "    yerr = np.vstack([lower_err, upper_err])\n",
    "    \n",
    "    x = np.arange(test_numstep + 1)\n",
    "    plt.errorbar(x, metaeval_mu, yerr=yerr, fmt='-o')\n",
    "    plt.fill_between(x, np.maximum(0, metaeval_mu - metaeval_ci), metaeval_mu + metaeval_ci, alpha=0.2)\n",
    "    \n",
    "    plt.xlabel('Gradient Steps')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('Meta-evaluation Loss vs Gradient Steps (Deterministic LPN)')\n",
    "    plt.show()\n",
    "    \n",
    "    return metaeval_losses, metaeval_mu, metaeval_ci\n",
    "\n",
    "# Visualization function\n",
    "def visualize_deterministic_lpn(lpn, device):\n",
    "    # Generate test data\n",
    "    test_xs, test_ys, amp, phase = SineDataGenerator(10, 1).generate()\n",
    "    \n",
    "    amp = amp.item()\n",
    "    phase = phase.item()\n",
    "    \n",
    "    io_pairs_t = torch.cat([test_xs, test_ys], dim=-1).to(device)\n",
    "    aux_t, loss_t = lpn(io_pairs_t, K=200, debug=True)\n",
    "    \n",
    "    y_pred = aux_t[\"ys_pred\"].detach().cpu().numpy()\n",
    "    \n",
    "    # Note: Different key names compared to the probabilistic version\n",
    "    z = aux_t[\"z\"].detach() if \"z\" in aux_t else None\n",
    "    z_prime = aux_t[\"z_prime\"].detach()\n",
    "    \n",
    "    # Plot visualization\n",
    "    x = np.linspace(-5, 5, 200)\n",
    "    \n",
    "    for i in range(10):\n",
    "        # Plot ground-truth\n",
    "        y = amp * np.sin(x - phase)\n",
    "        plt.plot(x, y, label=f\"ground-truth\", color='blue', alpha=.5, zorder=20)\n",
    "        \n",
    "        # Plot samples\n",
    "        plt.scatter(test_xs, test_ys, label=f\"samples\", color='blue', marker='s', zorder=30)\n",
    "        \n",
    "        # Plot target x,y (sample i of batch 0), and its prediction\n",
    "        x_i = test_xs[0, i].item()\n",
    "        y_i = test_ys[0, i].item()\n",
    "        yhat_i = y_pred[0, i].item()\n",
    "        plt.scatter(x_i, y_i, marker='s', color='green', label=f\"target ground-truth\", zorder=60)\n",
    "        plt.scatter(x_i, yhat_i, marker='o', color='k', label=f\"target prediction\", zorder=70)\n",
    "        \n",
    "        # Plot the predicted latent program\n",
    "        x_prep = torch.Tensor(x).unsqueeze(1).to(device)  # (200, 1)\n",
    "        \n",
    "        # Plot using initial z if available\n",
    "        if z is not None:\n",
    "            z_i = z[0, i]\n",
    "            y_pred_z_i = lpn.decode(z_i, x_prep).detach().cpu().numpy()\n",
    "            plt.plot(x, y_pred_z_i, label=\"predictions (by initial z)\",\n",
    "                     color='orange', linestyle='dashed', alpha=1, zorder=45)\n",
    "        \n",
    "        # Plot using optimized z_prime\n",
    "        z_pr_i = z_prime[0, i]\n",
    "        y_pred_z_pr_i = lpn.decode(z_pr_i, x_prep).detach().cpu().numpy()\n",
    "        plt.plot(x, y_pred_z_pr_i, label=\"predictions (by optimized z')\",\n",
    "                 color='k', linestyle='dashed', alpha=1, zorder=50)\n",
    "        \n",
    "        plt.xlabel(\"Inputs x\")\n",
    "        plt.ylabel(\"Outputs y\")\n",
    "        plt.title(f\"Deterministic LPN Predictions (Sample {i+1})\")\n",
    "        plt.legend(loc='center left', bbox_to_anchor=(1, 0.5))\n",
    "        plt.tight_layout()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3297e655",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Updated cross-validation function with data generation from the SineDataGenerator class\n",
    "\n",
    "def cross_validate_lpn_with_training_and_evaluation_updated(configs, device):\n",
    "    results = []\n",
    "    \n",
    "    # Create data generator for training\n",
    "    n_samples_per_task = 20\n",
    "    batch_size = 25\n",
    "    data_gen = SineDataGenerator(n_samples_per_task, batch_size)\n",
    "\n",
    "    # K-Fold Cross Validation setup\n",
    "    encoder_layer_configs = configs[\"encoder_layer_configs\"]\n",
    "    decoder_layer_configs = configs[\"decoder_layer_configs\"]\n",
    "    d_latent = configs[\"d_latent\"]\n",
    "    \n",
    "    for encoder_config in encoder_layer_configs:\n",
    "        for decoder_config in decoder_layer_configs:\n",
    "            # Initialize the model with the current configuration\n",
    "            encoder = DeterministicIOPairSetEncoder(**encoder_config)\n",
    "            decoder = Decoder(d_input=1, d_output=1, d_latent=d_latent, ds_hidden = decoder_config)\n",
    "            lpn = DeterministicLPN(d_input=1, d_output=1, d_latent=d_latent, encoder=encoder, decoder=decoder).to(device)\n",
    "            \n",
    "            val_mu = []\n",
    "            val_ci = []\n",
    "            \n",
    "            # increase this to do CV\n",
    "            for i in range(1):  \n",
    "\n",
    "                data_gen_train = SineDataGenerator(num_samples_per_class=n_samples_per_task, batch_size=batch_size)\n",
    "\n",
    "                # Train the model\n",
    "                train_deterministic_lpn(lpn, data_gen_train, device)\n",
    "\n",
    "                # Evaluation after training\n",
    "                metaeval_losses, metaeval_mu, metaeval_ci = evaluate_deterministic_lpn(lpn, device)\n",
    "                val_mu.append(metaeval_mu)\n",
    "                val_ci.append(metaeval_ci)\n",
    "            val_mu_mean = np.mean(val_mu, axis=0)\n",
    "            val_ci_mean = np.mean(val_ci, axis=0)\n",
    "            # Record results for this configuration\n",
    "            results.append({\n",
    "                'encoder_config': encoder_config,\n",
    "                'decoder_config': decoder_config,\n",
    "                'latent_dim': 3,\n",
    "                'mu': val_mu_mean,\n",
    "                'ci': val_ci\n",
    "            })\n",
    "    \n",
    "    return results\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49628973",
   "metadata": {},
   "outputs": [],
   "source": [
    "d_latent = 2\n",
    "encoder_config = [\n",
    "                   {\"phi\": ReluNet(2, 16, 16), \"rho_0\": ReluNet(16, 16, 8), \"rho_1\": ReluNet(8, 4, d_latent)}]\n",
    "\n",
    "\n",
    "decoder_config = [[16,16], [32,32] ]\n",
    "\n",
    "# Run the cross-validation with grid search\n",
    "results = cross_validate_lpn_with_training_and_evaluation_updated(configs={'encoder_layer_configs':\n",
    "    encoder_config, 'decoder_layer_configs': decoder_config, \"d_latent\": d_latent}, \n",
    "    device=torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"))\n",
    "\n",
    "# Display results of the grid search\n",
    "df_results = pd.DataFrame(results)\n",
    "df.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lpn_py310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
